apiVersion: v1
kind: ConfigMap
metadata:
  name: modelprovisioner-config
data:
  config.yaml: |
    litellm:
      url: http://litellm.api.svc:4000
    backends:
      - name: ollama
        url: http://ollama.llm.svc:11434/v1
        overrides:
          - regex: ".*vision.*"
            capabilities:
              supports_vision: true
          - regex: ".*tool.*"
            capabilities:
              supports_function_calling: true
        model_info_defaults:
          max_tokens: 4096
          temperature: 0.7
        litellm_params_defaults:
          tenant_id: "foo"
      - name: ollama2
        url: http://ollama2.llm.svc:11434/v1
        discovery: true
      - name: openrouter
        url: https://openrouter.ai/api/v1
        filter_regex: ".*:free$"
        model_info_defaults:
          some_key: "some value"
        litellm_params_defaults:
          some_other_key: 42
      - name: vllm1
        type: vllm
        url: http://my-vllm-endpoint.llm.svc:8000
        models_endpoint: /v1/models
        model_format: "hosted_vllm/{model}"
        discovery: true
      - name: ollama3
        type: ollama
        url: http://ollama2.llm.svc:11434
        generic_params:
          api_key: "another-special-key"
          custom_header: "another-value"